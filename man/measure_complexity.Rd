% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/complexity.R
\name{measure_complexity}
\alias{measure_complexity}
\title{Measure shapefile complexity}
\usage{
measure_complexity(shapefile)
}
\arguments{
\item{shapefile}{sf object (must be in EPSG:5070 for accurate estimates)}
}
\value{
List with 4 elements:
\describe{
\item{n_vertices}{Integer. Total number of vertices (coordinate points).}
\item{n_parts}{Integer. Number of polygon parts (e.g., 5 for 5-part multipolygon).}
\item{estimated_time_sec}{Numeric. Estimated processing time in seconds.
Based on empirical benchmarks with WRI_score.tif.}
\item{complexity_level}{Character. "low", "medium", or "high".}
}
}
\description{
Counts vertices and polygon parts to assess how complex a shapefile is.
More vertices = longer processing time for crop/mask operations.
}
\details{
\strong{Complexity levels:}
\itemize{
\item \strong{low} (<1,000 vertices): Fast processing (<5 seconds)
\item \strong{medium} (1,000-10,000 vertices): Moderate (5-30 seconds)
\item \strong{high} (>10,000 vertices): Slow (30+ seconds, possibly minutes)
}

\strong{Time estimates:}
Based on benchmarks with WRI_score.tif (3.5 GB, 90m resolution):
\itemize{
\item 100 vertices: ~0.5 seconds
\item 1,000 vertices: ~2 seconds
\item 10,000 vertices: ~20 seconds
\item 50,000 vertices: ~180 seconds (3 minutes)
}

Time scales roughly linearly with vertex count. Your mileage may vary based
on CPU speed, network bandwidth (if streaming COG), and polygon shape.
}
\examples{
\dontrun{
# Simple polygon
simple_poly <- sf::st_as_sf(sf::st_sfc(
  sf::st_buffer(sf::st_point(c(-2000000, 1500000)), 50000),
  crs = 5070
))
measure_complexity(simple_poly)
# n_vertices: ~100, complexity_level: "low"

# Complex high-resolution boundary
complex_poly <- sf::st_read("high_res_watershed.shp")
complex_poly <- sf::st_transform(complex_poly, 5070)
measure_complexity(complex_poly)
# n_vertices: 25,000, complexity_level: "high"
}

}
